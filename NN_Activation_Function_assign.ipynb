{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286ec81-80c3-4880-9989-9b17f336ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "    Ans: An activation function in artificial neural networks is like an on/off switch for a neuron. It determines if the neuron \n",
    "         should \"fire\" and pass information to the next layer. It introduces non-linearity, enabling the network to learn complex \n",
    "         patterns and make accurate predictions in various tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b03b78-9ec5-448a-871a-ace7a8f513da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "    Ans: Sigmoid Function: It squashes the input into a range between 0 and 1, useful for binary classification tasks.\n",
    "         ReLU (Rectified Linear Unit): It sets negative values to zero and keeps positive values as they are, providing \n",
    "             faster training and avoiding the vanishing gradient problem.\n",
    "         Leaky ReLU: Similar to ReLU, but allows a small negative slope for negative inputs to mitigate the \"dying ReLU\" problem.\n",
    "         Tanh (Hyperbolic Tangent): Similar to the sigmoid, but squashes input into a range between -1 and 1, making it zero-centered.\n",
    "         Softmax: Used in the output layer for multi-class classification, it converts raw scores into probability distributions, \n",
    "             ensuring the sum of probabilities equals 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93157fb8-57a6-454b-8394-ce0be56d7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "    Ans: Activation functions introduce non-linearity into the neural network, which allows it to learn more complex patterns in the data.\n",
    "         Proper activation functions enable the network to learn complex patterns, prevent vanishing gradients, and improve training speed. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98595a-7e56-49de-ac28-1407e31230d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "    Ans: The sigmoid activation function is a mathematical function that takes any real number as input and outputs a value between 0 and 1. \n",
    "         It is often used in neural networks as a way to introduce non-linearity into the network.\n",
    "         Advantages: It's easy to understand and historically used in binary classification tasks. \n",
    "         Disadvantages: It can cause vanishing gradients, slowing down learning in deep networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1a4c3-9300-4f23-8c3d-600e649a2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "    Ans: ReLU (Rectified Linear Unit) is an activation function that outputs the input directly if it's positive, and zero if it's negative. \n",
    "         It's faster and helps prevent vanishing gradients compared to the sigmoid function, which squashes input to a range between 0 and 1, \n",
    "         making ReLU more suitable for deep neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a9a4c-52b7-42d2-9eb9-dd2272004389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "    Ans: Using ReLU over the sigmoid function offers faster training in deep neural networks because ReLU doesn't suffer from the \n",
    "         vanishing gradient problem. ReLU directly outputs positive inputs, avoiding unnecessary computations and enabling the network \n",
    "         to learn complex patterns more efficiently.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab05ed9-bca2-4d16-8ce5-9673ec8205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "    Ans: Leaky ReLU is a variation of the ReLU activation function that allows a small negative slope for negative inputs, \n",
    "         preventing the vanishing gradient problem. It ensures that even when the input is negative, the neuron still has a \n",
    "         small signal to learn from, helping to maintain efficient training in deep neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eb61b-f53b-49ee-a9e7-0e97c2af99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "    Ans: The purpose of the softmax activation function is to convert raw scores into probabilities, ensuring the sum of probabilities \n",
    "         equals 1. It's commonly used in the output layer of a neural network for multi-class classification tasks, where the model \n",
    "         needs to assign the most likely class label among multiple classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aada5cc-2186-47d5-8584-06b062df61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "    Ans: The tanh activation function is similar to the sigmoid function, but it has a wider range of outputs (-1 to 1). \n",
    "         Tanh is less prone to overfitting than sigmoid, but it is also less easy to understand and implement.\n",
    "         It's similar to the sigmoid but has a larger output range, making it better for training deep networks \n",
    "         as it helps alleviate the vanishing gradient problem.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
